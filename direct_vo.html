<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,400,900">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans:300,400,700">
    <link rel="stylesheet" href="../../latex.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
        },
        TeX: {
          equationNumbers: {
            autoNumber: "AMS",
            useLabelIds: true
          }
        },
        "HTML-CSS": {
          availableFonts: ["STIX"],
          linebreaks: { automatic: true },
          imageFont: null
        }
      });
    </script>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <meta charset="UTF-8">
    <base target="_blank">
    <title>Direct Visual Odometry</title>
    <style>
        body {
            font-family: 'Lato', 'Google Sans', sans-serif;
            line-height: 1.6;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px 40px;
            font-size: 20px;
            counter-reset: figure-counter;
            text-align: justify;
        }
        h1 {
            color: #333;
            font-family: 'Google Sans', sans-serif;
            text-align: center;
            margin-bottom: 2em;
            font-size: 2.5em;
        }
        h2 {
            color: #333;
            font-family: 'Google Sans', sans-serif;
            font-size: 1.8em;
            margin-top: 1.5em;
        }
        h3 {
            color: #333;
            font-family: 'Google Sans', sans-serif;
            font-size: 1.4em;
            margin-top: 1.2em;
        }
        .figure {
            text-align: center;
            margin: 30px 0;
            width: 100%;
            counter-increment: figure-counter;
        }
        .figure img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 0 auto;
        }
        .figure-caption {
            font-style: italic;
            margin-top: 15px;
            text-align: center;
            font-size: 1em;
        }
        .figure-caption::before {
            content: "Figure " counter(figure-counter) ": ";
            font-weight: bold;
            font-style: normal;
        }
        .subfigure-container {
            display: flex;
            justify-content: center;
            gap: 40px;
            margin-bottom: 20px;
            max-width: 1400px;
            margin-left: auto;
            margin-right: auto;
        }
        .subfigure {
            flex: 0 1 600px;
            text-align: center;
        }
        .subfigure img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 0 auto;
            min-width: 400px;
        }
        .subfigure .figure-caption {
            font-style: italic;
            margin-top: 10px;
            text-align: center;
            font-size: 1em;
        }
        .subfigure .figure-caption::before {
            content: none;
        }
        .MathJax {
            font-size: 1.1em !important;
        }
        .MathJax_Display {
            overflow-x: auto;
            overflow-y: hidden;
            margin: 1em 0;
        }
        .equation-container {
            display: table;
            width: 100%;
            margin: 1.5em 0;
        }
        .equation-content {
            display: table-cell;
            width: 100%;
        }
        .figure-label::before {
            content: "Figure " counter(figure-counter) ":";
            font-weight: bold;
            margin-right: 0.5em;
        }
        /* MathJax display styles */
        .MJXc-display {
            overflow-x: auto;
            overflow-y: hidden;
            scrollbar-width: none;
            -ms-overflow-style: none;
        }
        .MJXc-display::-webkit-scrollbar {
            width: 5px;
            height: 2px;
        }
        .MJXc-display::-webkit-scrollbar-track {
            background: transparent;
        }
        .MJXc-display::-webkit-scrollbar-thumb {
            background: #ddd;
            visibility:hidden;
        }
        .MJXc-display:hover::-webkit-scrollbar-thumb {
            visibility:visible;
        }
    </style>
</head>
<body>
<h1>Direct Visual Odometry</h1>

<h2>Photometric loss and its optimization problem</h2>

<h3>the formulation of the photometric loss</h3>
<!-- Given a 3D point $\mathbf{X}_W$ in world frame, we can project it to the image plane as $\mathbf{x}$,
\begin{equation}
\mathbf{X}_W = [\,X,\;Y,\;Z\,]^{\top}
\end{equation} -->

Given two camera frames $\mathcal{F}_{ref}$ and $\mathcal{F}_{tgt}$, we find an implicit 2D correspondence $\mathbf{x}_{ref}$ and $\mathbf{x}_{tgt}$ with estimated depth in reference frame, denoted as $\hat{Z}_{ref}$ or just $\hat{Z}$, and transform of these two frames $\hat{\mathbf{T}}_{tgt\_ref}$ (equivalent to $\hat{\mathbf{T}}_{CW}$, reference frame expressed in target frame).

Based on a estimated depth $\hat{Z}$, we can project the 2D point and get the estimated 3D point $\hat{\mathbf{X}}_C$ in camera frame,
\begin{equation}
\hat{\mathbf{X}}_C = \hat{Z}\,\mathbf{K}^{-1}\ \mathbf{x}_{ref}
\end{equation}

Then transform the point based on estimated pose,
\begin{equation}
\hat{\mathbf{X}}_{W} = \hat{\mathbf{T}}_{tgt\_ref} \left( \hat{Z}\,\mathbf{K}^{-1}\ \mathbf{x}_{ref} \right)
\end{equation}

and then the warped 2D image point,
\begin{equation}
\mathbf{x}_{wp} = \mathbf{K} \hat{\mathbf{T}}_{tgt\_ref} \left(\hat{Z} \mathbf{K}^{-1} \mathbf{x}_{ref} \right)
\end{equation}

If the estimated depth in ref frame $\hat{Z}$, and the estimated pose $\hat{\mathbf{T}}_{tgt\_ref}$ are all perfectly accurate, then the warped image point $\mathbf{x}_{wp}$ should exactly align with the 2D correpondence of $\mathbf{x}_{ref}$, hence $\mathbf{x}_{tgt}$. But this is not the case, then we have an optimization problem to optimize $\hat{Z}$ and $\hat{\mathbf{T}}_{tgt\_ref}$, based on the pixel intensity of the points $\mathbf{x}_{wp}$ and $\mathbf{x}_{tgt}$, denoted as $I_{\mathrm{wp}}$, $I_{\mathrm{tgt}}$, respectively. The loss function is called $\textit{photometric loss}$, which is formulated as,
\begin{equation}
J = \frac{1}{N}\sum_{i=1}^{N}
\left\| I_{\mathrm{tgt}}(i) - I_{\mathrm{wp}}(i)\,\right\|^2 = \frac{1}{N}\sum_{i=1}^{N} \left\| e_i \right\|^2
\end{equation}

<!-- \min_{ \{ \hat{\mathbf{T}}_{tgt\_ref}, \hat{Z} \} }  -->

where $N$ denotes the total number of points.

For RGBD and stereo camera rig, it is easy to direct compute the depth map $\hat{Z}$ from the depth sensor. But for monocular cameras, the depth needs to be estimated. <br>

<h3> pose and depth optimization </h3>
 We first derive the gradient of error term $e$ w.r.t pose, assuming constant depth $\hat{Z}$,

where,
\begin{equation}
\mathbf{x}_{ref}= [u_1, v_1, 1 ]^\top
\qquad
\mathbf{x}_{tgt} = [u_2, v_2, 1 ]^\top
\end{equation}

Consider the left perturbation model of Lie algebra, using the first-order Taylor expansion:
Here we denote the transformation $\hat{\mathbf{T}}_{tgt\_ref}$ simply as $\hat{\mathbf{T}}$,
\begin{equation}
e(\mathbf{\hat{T}}) = I_{\mathrm{tgt}}(\mathbf{x}_{tgt}) - I_{\mathrm{wp}}(\mathbf{x}_{wp})
\end{equation}

Then we derive the gradient of this error term w.r.t. the pose, but instead of computing the derivative on SE(3), we only do it in its lie algebra $\mathfrak{se}(3)$, hence,

\begin{equation}
\frac{\partial e}{\partial \delta \boldsymbol{\xi}}
= - \frac{\partial I_{wp}}{\partial \mathbf{x}_{wp}}
\frac{\partial \mathbf{x}_{wp}}{\partial \hat{\mathbf{X}}_C}
\frac{\partial \hat{\mathbf{X}}_C}{\partial \delta \boldsymbol{\xi}},
\end{equation}

where $\delta \boldsymbol{\xi} \in \mathfrak{se}(3)$ is the left disturbance of $\mathbf{\hat{T}}$, where

$\frac{\partial I_{wp}}{\partial \mathbf{x}_{wp}}$ is the grayscale gradient at pixel coordinate $\mathbf{x}_{wp}$.

$\frac{\partial \mathbf{x}_{wp}}{\partial \hat{\mathbf{X}}_C}$ is the derivative of the projection equation with respect to the three-dimensional point in the camera frame. Let $\hat{\mathbf{X}}_C = [X, Y, Z]^{\top}$:
\begin{equation}
\frac{\partial \mathbf{x}_{wp}}{\partial \hat{\mathbf{X}}_C} =
\begin{bmatrix}
\frac{f_x}{Z} & 0 & -\frac{f_x X}{Z^2} \\
0 & \frac{f_y}{Z} & -\frac{f_y Y}{Z^2}
\end{bmatrix}.
\end{equation}

$\frac{\partial \hat{\mathbf{X}}_C}{\partial \delta \boldsymbol{\xi}}$ is the derivative of the transformed three-dimensional point with respect to the transformation, which was introduced before as,

\begin{equation}
\frac{\partial \hat{\mathbf{X}}_C}{\partial \delta \boldsymbol{\xi}} =
[\mathbf{I}, -(\hat{\mathbf{X}}_C)^{\wedge}].
\end{equation}

We can combine them together as:

\begin{equation}
\frac{\partial \mathbf{x}_{wp}}{\partial \delta \boldsymbol{\xi}} =
\begin{bmatrix}
\frac{f_x}{Z} & 0 & -\frac{f_x X}{Z^2} & -\frac{f_x X Y}{Z^2} & f_x + \frac{f_x X^2}{Z^2} & -\frac{f_x Y}{Z} \\
0 & \frac{f_y}{Z} & -\frac{f_y Y}{Z^2} & -f_y - \frac{f_y Y^2}{Z^2} & \frac{f_y X Y}{Z^2} & \frac{f_y X}{Z}
\end{bmatrix}.
\end{equation}

This $2 \times 6$ matrix also appeared in the previous post. Therefore, we derive the Jacobian of residual with respect to Lie algebra as:

\begin{equation}
\mathbf{J} = - \frac{\partial I_{wp}}{\partial \mathbf{x}_{wp}}
\frac{\partial \mathbf{x}_{wp}}{\partial \delta \boldsymbol{\xi}}.
\end{equation}

Based on Gauss-Newton methods, the incremental update of the pose is given as,
\begin{equation}
\mathbf{J}^\top \mathbf{J} \delta \boldsymbol{\xi} = \mathbf{J}^\top e \\
\delta \boldsymbol{\xi} = (\mathbf{J}^\top \mathbf{J} )^{-1} \mathbf{J}^\top e
\end{equation}

Following the left-multiplicative composition rule, The pose is then updated as,
\begin{equation} 
\hat{\mathbf{T}}_{k+1} = \exp \left( (\delta \boldsymbol{\xi})^\wedge \right) \hat{\mathbf{T}}_{k}
\end{equation}

where $k$ is the iteration step. <br>

This Jacobian is used in to gradient-based optimization problem to update the camera pose. From the above equation we can see that that gradient is 0 when the pixel gradient is 0, which means feature-less regions will have 0 gradient, this explain why classical direct methods like LSD-SLAM and DSO choose image regions with rich textures. 

<!-- Let's first derive the gradient of the optimization objective with respect to the depth map $\hat{Z}$. We leave the gradient with respect to the camera pose $\hat{\mathbf{T}}$ for the next section.

\begin{equation}
\begin{aligned}
\frac{\partial e}{\partial \hat{Z}} &= - \frac{\partial I_{\mathrm{warp}}}{\partial \hat{Z}} = - \frac{\partial I_{\mathrm{warp}}}{\partial \mathbf{x}_{warp}}  \frac{\partial \mathbf{x}_{warp}}{\partial \hat{Z}} \\
&= - \frac{\partial I_{\mathrm{warp}}}{\partial \mathbf{x}_{warp}} \left(\mathbf{K}\,[\,\hat{\mathbf{R}}\mid \hat{\mathbf{t}}\,]\,
\,\mathbf{K}^{-1}\, \mathbf{x}_{\mathrm{ref}} \right)
\end{aligned}
\end{equation}

Typical method of dense direct methods are LSD-SLAM and DSO. And in these methods only chunks of images which have high gradients will be used, such as corners and edges. -->


<h2>Sparse Direct Visual Odometry</h2>
In sparse direct methods, we also optimize the camera pose to minimize the photometric error, but insteand of using the whole image or a alrge chunk of image, we only use points from the keypoint and optical flow tracking (e.g. KLT). Here the photometric error is the brightness error of the two pixels of $\mathbf{X}$ projrected to two images:

The previous introduced feature-based visual odometry methods are mostly always sparse due to the extraction of keypoint and point features in images. In sparse direct visual odometry, we also track sparse keypoints in images, but instead of using features, we use the pixel intensity values directly. This will apply the sparse optical flow we introcued previously, e.g. Lucas-Kanade method.

<h2>Dense Direct Visual Odometry</h2>
In a dense direct visual odometry, we ultilize the pixel intensity in the entire image. The first steps of direct VO is to have an initial guess of current reference depth map $\hat{Z}$ and camera pose $\hat{\mathbf{T}} = [\hat{\mathbf{R}},\hat{\mathbf{t}}]$. Then we warp the reference image to the target frame using the estimated camera pose and depth map, and compute the photometric loss between the warped image and the target image, to optimize the camera pose and depth map.
