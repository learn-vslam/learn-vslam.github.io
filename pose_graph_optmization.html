<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,400,900">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans:300,400,700">
    <link rel="stylesheet" href="../../latex.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
        },
        TeX: {
          equationNumbers: {
            autoNumber: "AMS",
            useLabelIds: true
          }
        },
        "HTML-CSS": {
          availableFonts: ["STIX"],
          linebreaks: { automatic: true },
          imageFont: null
        }
      });
    </script>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <meta charset="UTF-8">
    <base target="_blank">
    <title>Pose Graph Optimization</title>
    <style>
        body {
            font-family: 'Lato', 'Google Sans', sans-serif;
            line-height: 1.6;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px 40px;
            font-size: 20px;
            counter-reset: figure-counter;
            text-align: justify;
        }
        h1 {
            color: #333;
            font-family: 'Google Sans', sans-serif;
            text-align: center;
            margin-bottom: 2em;
            font-size: 2.5em;
        }
        h2 {
            color: #333;
            font-family: 'Google Sans', sans-serif;
            font-size: 1.8em;
            margin-top: 1.5em;
        }
        h3 {
            color: #333;
            font-family: 'Google Sans', sans-serif;
            font-size: 1.4em;
            margin-top: 1.2em;
        }
        .figure {
            text-align: center;
            margin: 30px 0;
            width: 100%;
            counter-increment: figure-counter;
        }
        .figure img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 0 auto;
        }
        .figure-caption {
            font-style: italic;
            margin-top: 15px;
            text-align: center;
            font-size: 1em;
        }
        .figure-caption::before {
            content: "Figure " counter(figure-counter) ": ";
            font-weight: bold;
            font-style: normal;
        }
        .subfigure-container {
            display: flex;
            justify-content: center;
            gap: 40px;
            margin-bottom: 20px;
            max-width: 1400px;
            margin-left: auto;
            margin-right: auto;
        }
        .subfigure {
            flex: 0 1 600px;
            text-align: center;
        }
        .subfigure img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 0 auto;
            min-width: 400px;
        }
        .subfigure .figure-caption {
            font-style: italic;
            margin-top: 10px;
            text-align: center;
            font-size: 1em;
        }
        .subfigure .figure-caption::before {
            content: none;
        }
        .MathJax {
            font-size: 1.1em !important;
        }
        .MathJax_Display {
            overflow-x: auto;
            overflow-y: hidden;
            margin: 1em 0;
        }
        .equation-container {
            display: table;
            width: 100%;
            margin: 1.5em 0;
        }
        .equation-content {
            display: table-cell;
            width: 100%;
        }
        .figure-label::before {
            content: "Figure " counter(figure-counter) ":";
            font-weight: bold;
            margin-right: 0.5em;
        }
        /* MathJax display styles */
        .MJXc-display {
            overflow-x: auto;
            overflow-y: hidden;
            scrollbar-width: none;
            -ms-overflow-style: none;
        }
        .MJXc-display::-webkit-scrollbar {
            width: 5px;
            height: 2px;
        }
        .MJXc-display::-webkit-scrollbar-track {
            background: transparent;
        }
        .MJXc-display::-webkit-scrollbar-thumb {
            background: #ddd;
            visibility:hidden;
        }
        .MJXc-display:hover::-webkit-scrollbar-thumb {
            visibility:visible;
        }
    </style>
</head>
<body>
<h1>Pose Graph Optimization 1</h1>
From this tutorial, we will start to explore the back-end of visual SLAM. We will start from the optimization-based methods, which is usually formulated as a pose graph optimization problem.

<h2>The sparse bundle adjustment (BA) problem</h2>
In the bundle adjustment problem, we want to optimize the camera poses and the 3D points by minimizing the reprojection error, which is denoted as,
\begin{equation}
\begin{aligned}
& \arg \min_{\mathbf{T}_{CWi}, \mathbf{X}_{Wj}} \left( \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{n}\left\| \mathbf{x}_j - \mathbf{K} \mathbf{T}_{CWi} \mathbf{X}_{Wj} \right\|_2^2 \right) \\
&= \arg \min_{\mathbf{T}_{CWi}, \mathbf{X}_{Wj}} \left( \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{n}\left\| \mathbf{x}_j - \pi (\mathbf{T}_{CWi}, \mathbf{X}_{Wj}) \right\|_2^2 \right) \\
&= \arg \min_{\mathbf{T}_{CWi}, \mathbf{X}_{Wj}} f = \arg \min_{\mathbf{T}_{CWi}, \mathbf{X}_{Wj}} \left( \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{n} \mathbf{e}_{ij}^2 \right) \\
\label{eq:reproj_error}
\end{aligned}
\end{equation}

The variables we want to optimize are the camera poses and the 3D points, which are denoted as:
\begin{equation}
\mathbf{\mathcal{X}} = [\mathbf{T}_1, \ldots, \mathbf{T}_m, \mathbf{X}_1, \ldots, \mathbf{X}_n]^\top
\end{equation}

Recall the Gaussian-Newton method for solving the non-linear least squares problem, where we linearlize the objective function $f(\mathbf{x})$, 
\begin{equation}
f(\mathbf{x} + \Delta\mathbf{x}) 
\approx f(\mathbf{x}) + \mathbf{J}(\mathbf{x})^\top \Delta\mathbf{x}
\end{equation}

and the optimal solution can be found by solving,
\begin{equation}
\Delta\mathbf{x}^* = 
\arg\min_{\Delta\mathbf{x}} 
\tfrac{1}{2}\|f(\mathbf{x}) + \mathbf{J}(\mathbf{x})^\top \Delta\mathbf{x}\|^2
\end{equation}

The optimal $\mathbf{x}$ is solved with its normal equation,
\begin{equation}
\mathbf{J}(\mathbf{x})^\top \mathbf{J}(\mathbf{x})\Delta\mathbf{x} 
= -\mathbf{J}(\mathbf{x})^\top f(\mathbf{x})
\end{equation}

\begin{equation}
\mathbf{H}\Delta\mathbf{x} = \mathbf{g}
\end{equation}

where $\mathbf{H}$ is the second-order approximation of the Hessian. <br><br>

Apply this to this BA problem, here we just $\mathbf{J}_C$ to denote the Jacobian over pose and $\mathbf{J}_P$ the Jacobian over 3D landmark points,
\begin{equation}
\begin{aligned}
\frac{1}{2}\| f(\mathbf{\mathcal{X}} + \Delta\mathbf{\mathcal{X}})\|^2 
&\approx \frac{1}{2}\sum_{i=1}^m \sum_{j=1}^n 
\|\mathbf{e}_{ij} + {\mathbf{J}_C}_{ij}\Delta\boldsymbol{\xi}_i + {\mathbf{J}_P}_{ij}\Delta\mathbf{X}_{j}\|^2 \\
&= \frac{1}{2}\|\mathbf{e} + \mathbf{J}_C\Delta \mathbf{\mathcal{X}_c} + \mathbf{J}^X\Delta\mathbf{\mathcal{X}_p}\|^2
\end{aligned}
\end{equation}

\begin{equation}
\mathbf{H}\Delta\mathbf{\mathcal{X}} = \mathbf{g}
\end{equation}

The Jacobian which combines $\mathbf{J}_C$ and $\mathbf{J}_P$ is denoted as,
\begin{equation}
\mathbf{J} = [\mathbf{J}_C, \mathbf{J}_P]^\top
\end{equation}

And and approximated Hessian matrix is,
\begin{equation}
\begin{aligned}
\mathbf{H} = \mathbf{J}^\top \mathbf{J} &= 
\begin{bmatrix}
{\mathbf{J}_C}^\top \mathbf{J}_C & {\mathbf{J}_C}^\top \mathbf{J}_P \\
{\mathbf{J}_P}^\top \mathbf{J}_C & {\mathbf{J}_P}^\top \mathbf{J}_P
\end{bmatrix} \\
&= \begin{bmatrix}
\mathbf{H}_{CC} & \mathbf{H}_{CP} \\
\mathbf{H}_{PC} & \mathbf{H}_{PP}
\end{bmatrix}
\end{aligned}
\end{equation}

<h2>Sparsity of the Hessian structure</h2>
In visual SLAM,  the local observability between camera poses and 3D points makes the Jacobian of the reprojection error term $\mathbf{e}$ w.r.t $\mathbf{\mathcal{X}}$ sparse. Think about a camera moving in a scene, only a subset of the 3D point map is observed by a subset of camera pose trajectory.

<h3>Hessian sparsity example</h3>
We will first go through a concrete example to illustrate the sparsity of the Hessian matrix. Consider the case where we have $M=2$ cameras and $N=4$ 3D landmark points. The illustration and its factor graph is shown as,

The reprojection error term is
\begin{equation}
\frac{1}{2} \sum_{i=1}^M \sum_{j=1}^N \|\mathbf{e}_{ij}\|^2 
\end{equation}

Take the Jacobian of $\mathbf{e}_{1}$ of $\mathbf{\mathcal{X}_{11}}$, hence the first pose and first 3D point, as the example,
\begin{equation}
\mathbf{J}_{11} =
\frac{\partial \mathbf{e}_{11}}{\partial \mathcal{X}} = \Big[ \tfrac{\partial \mathbf{e}_{11}}{\partial \boldsymbol{\xi}_1}, [\mathbf{0}]_{2\times6}, \tfrac{\partial \mathbf{e}_{11}}{\partial \mathbf{X}_1}, [\mathbf{0}]_{2\times3}, [\mathbf{0}]_{2\times3} \Big]
\end{equation}

\begin{equation}
\mathbf{J}_{12} =
\frac{\partial \mathbf{e}_{12}}{\partial \mathbf{\mathcal{X}}} = \Big[ \tfrac{\partial \mathbf{e}_{12}}{\partial \boldsymbol{\xi}_1}, [\mathbf{0}]_{2\times6}, [\mathbf{0}]_{2\times3}, \tfrac{\partial \mathbf{e}_{12}}{\partial \mathbf{X}_2}, [\mathbf{0}]_{2\times3} \Big]
\end{equation}

Since the 3rd landmark point is not observed by the 1st camera, $\mathbf{J}_{13}$ is actually not defined and we simply skip the computation of it. And then we continue for the 2nd camera,
\begin{equation}
\mathbf{J}_{22} =
\frac{\partial \mathbf{e}_{22}}{\partial \mathbf{\mathcal{X}}} = \Big[ [\mathbf{0}]_{2\times6}, \tfrac{\partial \mathbf{e}_{22}}{\partial \boldsymbol{\xi}_2}, [\mathbf{0}]_{2\times3}, \tfrac{\partial \mathbf{e}_{22}}{\partial \mathbf{X}_2}, [\mathbf{0}]_{2\times3} \Big]
\end{equation}

\begin{equation}
\mathbf{J}_{23} =
\frac{\partial \mathbf{e}_{23}}{\partial \mathbf{\mathcal{X}}} = \Big[ [\mathbf{0}]_{2\times6}, \tfrac{\partial \mathbf{e}_{23}}{\partial \boldsymbol{\xi}_2}, [\mathbf{0}]_{2\times3}, [\mathbf{0}]_{2\times3}, \tfrac{\partial \mathbf{e}_{23}}{\partial \mathbf{X}_3} \Big]
\end{equation}

stack them together and get the whole jacobian matrix,
\begin{equation}
\mathbf{J} = 
\begin{bmatrix}
\mathbf{J}_{11} \\
\mathbf{J}_{12} \\
\mathbf{J}_{22} \\
\mathbf{J}_{23} \\
\end{bmatrix}
\end{equation}

Thus the Hessian matrix $\mathbf{H}$ can be derived as,
\begin{equation}
\mathbf{H} = \mathbf{J}^\top \mathbf{J} = 
\begin{bmatrix}
\mathbf{J}_{11} ^\top, \mathbf{J}_{12} ^\top, \mathbf{J}_{22} ^\top, \mathbf{J}_{23} ^\top \\
\end{bmatrix} 
\begin{bmatrix}
\mathbf{J}_{11} \\
\mathbf{J}_{12} \\
\mathbf{J}_{22} \\
\mathbf{J}_{23} \\
\end{bmatrix} 
= 
\sum_{i,j} \mathbf{J}_{ij}^\top \mathbf{J}_{ij}
\end{equation}

\begin{equation}
\mathbf{J}_{11}^\top \mathbf{J}_{11} =
\begin{bmatrix}
\left( \frac{\partial \mathbf{e}_{11}}{\partial \boldsymbol{\xi}_1} \right)^\top \frac{\partial \mathbf{e}_{11}}{\partial \boldsymbol{\xi}_1}, [\mathbf{0}]_{6\times6}, \left( \frac{\partial \mathbf{e}_{11}}{\partial \boldsymbol{\xi}_1} \right)^\top \frac{\partial \mathbf{e}_{11}}{\partial \mathbf{X}_1}, [\mathbf{0}]_{3\times3}, [\mathbf{0}]_{3\times3} \\
\mathbf{0}  \\
\left( \frac{\partial \mathbf{e}_{11}}{\partial \mathbf{X}_1} \right)^\top \frac{\partial \mathbf{e}_{11}}{\partial \boldsymbol{\xi}_1}, [\mathbf{0}]_{3\times6}, \left( \frac{\partial \mathbf{e}_{11}}{\partial \boldsymbol{\xi}_1} \right)^\top \frac{\partial \mathbf{e}_{11}}{\partial \mathbf{X}_1}, [\mathbf{0}]_{3\times3}, [\mathbf{0}]_{3\times3} \\
\mathbf{0} \\
\mathbf{0} \\
\end{bmatrix}
\end{equation}

\begin{equation}
\mathbf{J}_{12}^\top \mathbf{J}_{12} =
\begin{bmatrix}
\left( \frac{\partial \mathbf{e}_{12}}{\partial \boldsymbol{\xi}_1} \right)^\top
\frac{\partial \mathbf{e}_{12}}{\partial \boldsymbol{\xi}_1}
, [\mathbf{0}]_{6\times6}
, [\mathbf{0}]_{6\times3}
, \left( \frac{\partial \mathbf{e}_{12}}{\partial \boldsymbol{\xi}_1} \right)^\top
\frac{\partial \mathbf{e}_{12}}{\partial \mathbf{X}_2}
, [\mathbf{0}]_{6\times3} \\
\mathbf{0} \\
\mathbf{0} \\
\left( \frac{\partial \mathbf{e}_{12}}{\partial \mathbf{X}_2} \right)^\top
\frac{\partial \mathbf{e}_{12}}{\partial \boldsymbol{\xi}_1}
, [\mathbf{0}]_{3\times6}
, [\mathbf{0}]_{3\times3}
, \left( \frac{\partial \mathbf{e}_{12}}{\partial \mathbf{X}_2} \right)^\top
\frac{\partial \mathbf{e}_{12}}{\partial \mathbf{X}_2}
, [\mathbf{0}]_{3\times3} \\
\mathbf{0}
\end{bmatrix}
\end{equation}

\begin{equation}
\mathbf{J}_{22}^\top \mathbf{J}_{22} =
\begin{bmatrix}
\mathbf{0}
\\
[\mathbf{0}]_{6\times6}
, \left( \frac{\partial \mathbf{e}_{22}}{\partial \boldsymbol{\xi}_2} \right)^\top
\frac{\partial \mathbf{e}_{22}}{\partial \boldsymbol{\xi}_2}
, [\mathbf{0}]_{6\times3}
, \left( \frac{\partial \mathbf{e}_{22}}{\partial \boldsymbol{\xi}_2} \right)^\top
\frac{\partial \mathbf{e}_{22}}{\partial \mathbf{X}_2}
, [\mathbf{0}]_{6\times3}
\\
\mathbf{0}
\\
[\mathbf{0}]_{3\times6}
, \left( \frac{\partial \mathbf{e}_{22}}{\partial \mathbf{X}_2} \right)^\top
\frac{\partial \mathbf{e}_{22}}{\partial \boldsymbol{\xi}_2}
, [\mathbf{0}]_{3\times3}
, \left( \frac{\partial \mathbf{e}_{22}}{\partial \mathbf{X}_2} \right)^\top
\frac{\partial \mathbf{e}_{22}}{\partial \mathbf{X}_2}
, [\mathbf{0}]_{3\times3}
\\
\mathbf{0}
\end{bmatrix}
\end{equation}


\begin{equation}
\mathbf{J}_{23}^\top \mathbf{J}_{23} =
\begin{bmatrix}
\mathbf{0} \\
[\mathbf{0}]_{6\times6}
,\left( \frac{\partial \mathbf{e}_{23}}{\partial \boldsymbol{\xi}_2} \right)^\top
\frac{\partial \mathbf{e}_{23}}{\partial \boldsymbol{\xi}_2}
, [\mathbf{0}]_{6\times3}
, [\mathbf{0}]_{6\times3}
, \left( \frac{\partial \mathbf{e}_{23}}{\partial \boldsymbol{\xi}_2} \right)^\top
\frac{\partial \mathbf{e}_{23}}{\partial \mathbf{X}_3}
\\
\mathbf{0}
\\
\mathbf{0}
\\
[\mathbf{0}]_{3\times6}
, \left( \frac{\partial \mathbf{e}_{23}}{\partial \mathbf{X}_3} \right)^\top
\frac{\partial \mathbf{e}_{23}}{\partial \boldsymbol{\xi}_2}
, [\mathbf{0}]_{3\times3}
, [\mathbf{0}]_{3\times3}
, \left( \frac{\partial \mathbf{e}_{23}}{\partial \mathbf{X}_3} \right)^\top
\frac{\partial \mathbf{e}_{23}}{\partial \mathbf{X}_3}
\end{bmatrix}
\end{equation}

We define derivative terms as,
\begin{equation}
\mathbf{E}_{ij}
\triangleq
\frac{\partial \mathbf{e}_{ij}}{\partial \boldsymbol{\xi}_i}
\in \mathbb{R}^{2\times 6},
\qquad
\mathbf{F}_{ij}
\triangleq
\frac{\partial \mathbf{e}_{ij}}{\partial \mathbf{X}_j}
\in \mathbb{R}^{2\times 3}.
\end{equation}


And the Hessian $\mathbf{H}$ is,
\begin{equation}
\mathbf{H}
=
\left[
\begin{array}{cc|ccc}
\mathbf{E}_{11}^\top\mathbf{E}_{11} + \mathbf{E}_{12}^\top\mathbf{E}_{12}
& [\mathbf{0}]_{6\times6}
& \mathbf{E}_{11}^\top\mathbf{F}_{11}
& \mathbf{E}_{12}^\top\mathbf{F}_{12}
& [\mathbf{0}]_{6\times3}
\\[6pt]

[\mathbf{0}]_{6\times6}
& \mathbf{E}_{22}^\top\mathbf{E}_{22} + \mathbf{E}_{23}^\top\mathbf{E}_{23}
& [\mathbf{0}]_{6\times3}
& \mathbf{E}_{22}^\top\mathbf{F}_{22}
& \mathbf{E}_{23}^\top\mathbf{F}_{23}
\\
\hline

\mathbf{F}_{11}^\top\mathbf{E}_{11}
& [\mathbf{0}]_{3\times6}
& \mathbf{F}_{11}^\top\mathbf{F}_{11}
& [\mathbf{0}]_{3\times3}
& [\mathbf{0}]_{3\times3}
\\[6pt]

\mathbf{F}_{12}^\top\mathbf{E}_{12}
& \mathbf{F}_{22}^\top\mathbf{E}_{22}
& [\mathbf{0}]_{3\times3}
& \mathbf{F}_{12}^\top\mathbf{F}_{12} + \mathbf{F}_{22}^\top\mathbf{F}_{22}
& [\mathbf{0}]_{3\times3}
\\[6pt]

[\mathbf{0}]_{3\times6}
& \mathbf{F}_{23}^\top\mathbf{E}_{23}
& [\mathbf{0}]_{3\times3}
& [\mathbf{0}]_{3\times3}
& \mathbf{F}_{23}^\top\mathbf{F}_{23}
\end{array}
\right]
\end{equation}

It is obvious that the Hessian matrix is symmetric. The upper left block of the Hessian matrix is diagonal and all about camera pose $\boldsymbol{\xi}$ and the lower right block is also diagonal and all about 3D points $\mathbf{X}$. 

The effective size of the Hessian used for optimization depends on the observability between camera poses and 3D points. In fact, the observability information is also hidden in $\mathbf{H}$ itself. Think about the last block in the first row block of $\mathbf{H}$, it is a $[\mathbf{0}]$ block because the 3rd 3D landmark is not observable by the 1st camera. Same for the 3rd block in the second row block, it is $[\mathbf{0}]$ because the 1st 3D point is not observable by camera 2. <br>

In summary, the Jacobian of the reprojection error over the $i$-th camera pose and the $j$-th 3D landmark points are,
\begin{equation}
\mathbf{J}_{ij}(\mathcal{X}) =
\Big[
\underbrace{[\mathbf{0}]_{2\times6}\;\cdots\;[\mathbf{0}]_{2\times6}}_{\text{poses } 1\ldots i-1}\;\;
\tfrac{\partial \mathbf{e}_{ij}}{\partial \boldsymbol{\xi}_i}\;\;
\underbrace{[\mathbf{0}]_{2\times6}\;\cdots\;[\mathbf{0}]_{2\times6}}_{\text{poses } i+1\ldots M}\;\;
\underbrace{[\mathbf{0}]_{2\times3}\;\cdots\;[\mathbf{0}]_{2\times3}}_{\text{landmarks } 1\ldots j-1}\;\;
\tfrac{\partial \mathbf{e}_{ij}}{\partial \mathbf{X}_j}\;\;
\underbrace{[\mathbf{0}]_{2\times3}\;\cdots\;[\mathbf{0}]_{2\times3}}_{\text{landmarks } j+1\ldots N}
\Big]
\end{equation}

<h3>Schur Complement</h3>
\begin{equation}
\begin{bmatrix}
\mathbf{B} & \mathbf{J}_P \\
{\mathbf{J}_P}^\top & \mathbf{C}
\end{bmatrix}
\begin{bmatrix}
\Delta\mathbf{x}_c \\ \Delta\mathbf{x}_p
\end{bmatrix}
=
\begin{bmatrix}
\mathbf{v} \\ \mathbf{w}
\end{bmatrix}
\end{equation}

\begin{equation}
\begin{bmatrix}
\mathbf{I} & - \mathbf{J}_P \mathbf{C}^{-1} \\
\mathbf{0} & \mathbf{I}
\end{bmatrix}
\begin{bmatrix}
\mathbf{B} & \mathbf{J}_P \\
{\mathbf{J}_P}^\top & \mathbf{C}
\end{bmatrix}
\begin{bmatrix}
\Delta\mathbf{x}_c \\ \Delta\mathbf{x}_p
\end{bmatrix}
=
\begin{bmatrix}
\mathbf{I} & -\mathbf{J}_P \mathbf{C}^{-1} \\
\mathbf{0} & \mathbf{I}
\end{bmatrix}
\begin{bmatrix}
\mathbf{v} \\ \mathbf{w}
\end{bmatrix}
\end{equation}

\begin{equation}
\begin{bmatrix}
\mathbf{B} - \mathbf{J}_P \mathbf{C}^{-1} {\mathbf{J}_P}^\top & \mathbf{0} \\
{\mathbf{J}_P}^\top & \mathbf{C}
\end{bmatrix}
\begin{bmatrix}
\Delta\mathbf{x}_c \\ \Delta\mathbf{x}_p
\end{bmatrix}
=
\begin{bmatrix}
\mathbf{v} - \mathbf{J}_P \mathbf{C}^{-1} \mathbf{w} \\ \mathbf{w}
\end{bmatrix}
\end{equation}

\begin{equation}
[\mathbf{B} - \mathbf{J}_P \mathbf{C}^{-1} {\mathbf{J}_P}^\top] \Delta\mathbf{x}_c 
= \mathbf{v} - \mathbf{J}_P \mathbf{C}^{-1} \mathbf{w}
\end{equation}