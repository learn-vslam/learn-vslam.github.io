<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,400,900">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans:300,400,700">
    <link rel="stylesheet" href="../../latex.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
        },
        TeX: {
          equationNumbers: {
            autoNumber: "AMS",
            useLabelIds: true
          }
        },
        "HTML-CSS": {
          availableFonts: ["STIX"],
          linebreaks: { automatic: true },
          imageFont: null
        }
      });
    </script>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <meta charset="UTF-8">
    <base target="_blank">
    <title>Pose Graph Optimization</title>
    <style>
        body {
            font-family: 'Lato', 'Google Sans', sans-serif;
            line-height: 1.6;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px 40px;
            font-size: 20px;
            counter-reset: figure-counter;
            text-align: justify;
        }
        h1 {
            color: #333;
            font-family: 'Google Sans', sans-serif;
            text-align: center;
            margin-bottom: 2em;
            font-size: 2.5em;
        }
        h2 {
            color: #333;
            font-family: 'Google Sans', sans-serif;
            font-size: 1.8em;
            margin-top: 1.5em;
        }
        h3 {
            color: #333;
            font-family: 'Google Sans', sans-serif;
            font-size: 1.4em;
            margin-top: 1.2em;
        }
        .figure {
            text-align: center;
            margin: 30px 0;
            width: 100%;
            counter-increment: figure-counter;
        }
        .figure img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 0 auto;
        }
        .figure-caption {
            font-style: italic;
            margin-top: 15px;
            text-align: center;
            font-size: 1em;
        }
        .figure-caption::before {
            content: "Figure " counter(figure-counter) ": ";
            font-weight: bold;
            font-style: normal;
        }
        .subfigure-container {
            display: flex;
            justify-content: center;
            gap: 40px;
            margin-bottom: 20px;
            max-width: 1400px;
            margin-left: auto;
            margin-right: auto;
        }
        .subfigure {
            flex: 0 1 600px;
            text-align: center;
        }
        .subfigure img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 0 auto;
            min-width: 400px;
        }
        .subfigure .figure-caption {
            font-style: italic;
            margin-top: 10px;
            text-align: center;
            font-size: 1em;
        }
        .subfigure .figure-caption::before {
            content: none;
        }
        .MathJax {
            font-size: 1.1em !important;
        }
        .MathJax_Display {
            overflow-x: auto;
            overflow-y: hidden;
            margin: 1em 0;
        }
        .equation-container {
            display: table;
            width: 100%;
            margin: 1.5em 0;
        }
        .equation-content {
            display: table-cell;
            width: 100%;
        }
        .figure-label::before {
            content: "Figure " counter(figure-counter) ":";
            font-weight: bold;
            margin-right: 0.5em;
        }
        /* MathJax display styles */
        .MJXc-display {
            overflow-x: auto;
            overflow-y: hidden;
            scrollbar-width: none;
            -ms-overflow-style: none;
        }
        .MJXc-display::-webkit-scrollbar {
            width: 5px;
            height: 2px;
        }
        .MJXc-display::-webkit-scrollbar-track {
            background: transparent;
        }
        .MJXc-display::-webkit-scrollbar-thumb {
            background: #ddd;
            visibility:hidden;
        }
        .MJXc-display:hover::-webkit-scrollbar-thumb {
            visibility:visible;
        }
    </style>
</head>
<body>
<h1>Factor Graph Optimization</h1>
From this tutorial, we will start to explore the back-end of visual SLAM. We will start from the optimization-based methods, which is usually formulated as a pose graph optimization problem.

<h2>The sparse bundle adjustment (BA) problem</h2>
In the bundle adjustment problem, we want to optimize the camera poses and the 3D points by minimizing the reprojection error, which is denoted as,
\begin{equation}
\begin{aligned}
& \arg \min_{\mathbf{T}_{CWi}, \mathbf{X}_{Wj}} \left( \frac{1}{2} \sum_{i=1}^{M} \sum_{j=1}^{N}\left\| \mathbf{x}_j - \mathbf{K} \mathbf{T}_{CWi} \mathbf{X}_{Wj} \right\|_2^2 \right) \\
&= \arg \min_{\mathbf{T}_{CWi}, \mathbf{X}_{Wj}} \left( \frac{1}{2} \sum_{i=1}^{M} \sum_{j=1}^{N}\left\| \mathbf{x}_j - \pi (\mathbf{T}_{CWi}, \mathbf{X}_{Wj}) \right\|_2^2 \right) \\
&= \arg \min_{\mathbf{T}_{CWi}, \mathbf{X}_{Wj}} f = \arg \min_{\mathbf{T}_{CWi}, \mathbf{X}_{Wj}} \left( \frac{1}{2} \sum_{i=1}^{M} \sum_{j=1}^{N} \mathbf{e}_{ij}^2 \right) \\
\label{eq:reproj_error}
\end{aligned}
\end{equation}

The variables we want to optimize are the camera poses and the 3D points, which are denoted as:
\begin{equation}
\mathbf{\mathcal{X}} = [\boldsymbol{\xi}_1, \ldots, \boldsymbol{\xi}_i, \ldots, \boldsymbol{\xi}_M, \mathbf{X}_1, \ldots, \mathbf{X}_j, \ldots, \mathbf{X}_N]^\top
\end{equation}

where $\boldsymbol{\xi}_i \in \mathfrak{se}(3)$ is the Lie algebra of the camera pose $\mathbf{T}_{CWi} \in \text{SE}(3)$. <br>

Recall the Gaussian-Newton method for solving the non-linear least squares problem, where we linearlize the objective function $f(\mathbf{x})$, 
\begin{equation}
f(\mathbf{x} + \Delta\mathbf{x}) 
\approx f(\mathbf{x}) + \mathbf{J}(\mathbf{x})^\top \Delta\mathbf{x}
\end{equation}

and the optimal solution can be found by solving,
\begin{equation}
\Delta\mathbf{x}^* = 
\arg\min_{\Delta\mathbf{x}} 
\tfrac{1}{2}\|f(\mathbf{x}) + \mathbf{J}(\mathbf{x})^\top \Delta\mathbf{x}\|^2
\end{equation}

The optimal $\mathbf{x}$ is solved with its normal equation,
\begin{equation}
\mathbf{J}(\mathbf{x})^\top \mathbf{J}(\mathbf{x})\Delta\mathbf{x} 
= -\mathbf{J}(\mathbf{x})^\top f(\mathbf{x})
\end{equation}

\begin{equation}
\mathbf{H}\Delta\mathbf{x} = \mathbf{g}
\end{equation}

where $\mathbf{H}$ is the second-order approximation of the Hessian. <br><br>

Apply this to the BA problem, here we just $\mathbf{J}_C$ to denote the Jacobian over pose and $\mathbf{J}_P$ the Jacobian over 3D landmark points,
\begin{equation}
\begin{aligned}
\frac{1}{2}\| f(\mathbf{\mathcal{X}} + \Delta\mathbf{\mathcal{X}})\|^2 
&\approx \frac{1}{2}\sum_{i=1}^M \sum_{j=1}^N 
\|\mathbf{e}_{ij} + {\mathbf{J}_C}_{ij}\Delta\boldsymbol{\xi}_i + {\mathbf{J}_P}_{ij}\Delta\mathbf{X}_{j}\|^2 \\
&= \frac{1}{2}\|\mathbf{e} + \mathbf{J}_C\Delta \mathbf{\mathcal{X}_c} + \mathbf{J}^X\Delta\mathbf{\mathcal{X}_p}\|^2
\end{aligned}
\end{equation}

The incremental updates for $\mathcal{X}$ is represented as
\begin{equation}
\mathbf{H}\Delta\mathbf{\mathcal{X}} = -\mathbf{J}^\top \mathbf{e}
\end{equation}

\begin{equation}
\Delta\mathbf{\mathcal{X}} = - \mathbf{H}^{-1} \mathbf{J}^\top \mathbf{e} = -( \mathbf{J}^\top \mathbf{J} )^{-1} \mathbf{J}^\top \mathbf{e}
\end{equation}

The Jacobian which combines $\mathbf{J}_C$ and $\mathbf{J}_P$ is denoted as,
\begin{equation}
\mathbf{J} = [\mathbf{J}_C, \mathbf{J}_P]^\top
\end{equation}

And and approximated Hessian matrix is,
\begin{equation}
\begin{aligned}
\mathbf{H} = \mathbf{J}^\top \mathbf{J} &= 
\begin{bmatrix}
{\mathbf{J}_C}^\top \mathbf{J}_C & {\mathbf{J}_C}^\top \mathbf{J}_P \\
{\mathbf{J}_P}^\top \mathbf{J}_C & {\mathbf{J}_P}^\top \mathbf{J}_P
\end{bmatrix} \\
&= \begin{bmatrix}
\mathbf{H}_{CC} & \mathbf{H}_{CP} \\
\mathbf{H}_{PC} & \mathbf{H}_{PP}
\end{bmatrix}
\end{aligned}
\end{equation}

<h2>Sparsity of the Hessian structure</h2>
In visual SLAM,  the local observability between camera poses and 3D points makes the Jacobian of the reprojection error term $\mathbf{e}$ w.r.t $\mathbf{\mathcal{X}}$ sparse. Think about a camera moving in a scene, only a subset of the 3D point map is observed by a subset of camera pose trajectory.

<h3>Hessian sparsity introduction</h3>
We will first go through a concrete example to illustrate the sparsity of the Hessian matrix. Consider the case where we have $M=2$ cameras and $N=4$ 3D landmark points. The illustration and its factor graph is shown as,

The reprojection error term is
\begin{equation}
\frac{1}{2} \sum_{i=1}^M \sum_{j=1}^N \|\mathbf{e}_{ij}\|^2 
\end{equation}

Take the Jacobian of $\mathbf{e}_{1}$ of $\mathbf{\mathcal{X}_{11}}$, hence the first pose and first 3D point, as the example,
\begin{equation}
\mathbf{J}_{11} =
\frac{\partial \mathbf{e}_{11}}{\partial \mathcal{X}} = \Big[ \tfrac{\partial \mathbf{e}_{11}}{\partial \boldsymbol{\xi}_1}, [\mathbf{0}]_{2\times6}, \tfrac{\partial \mathbf{e}_{11}}{\partial \mathbf{X}_1}, [\mathbf{0}]_{2\times3}, [\mathbf{0}]_{2\times3} \Big]
\end{equation}

\begin{equation}
\mathbf{J}_{12} =
\frac{\partial \mathbf{e}_{12}}{\partial \mathbf{\mathcal{X}}} = \Big[ \tfrac{\partial \mathbf{e}_{12}}{\partial \boldsymbol{\xi}_1}, [\mathbf{0}]_{2\times6}, [\mathbf{0}]_{2\times3}, \tfrac{\partial \mathbf{e}_{12}}{\partial \mathbf{X}_2}, [\mathbf{0}]_{2\times3} \Big]
\end{equation}

Since the 3rd landmark point is not observed by the 1st camera, $\mathbf{J}_{13}$ is actually not defined and we simply skip the computation of it. And then we continue for the 2nd camera,
\begin{equation}
\mathbf{J}_{22} =
\frac{\partial \mathbf{e}_{22}}{\partial \mathbf{\mathcal{X}}} = \Big[ [\mathbf{0}]_{2\times6}, \tfrac{\partial \mathbf{e}_{22}}{\partial \boldsymbol{\xi}_2}, [\mathbf{0}]_{2\times3}, \tfrac{\partial \mathbf{e}_{22}}{\partial \mathbf{X}_2}, [\mathbf{0}]_{2\times3} \Big]
\end{equation}

\begin{equation}
\mathbf{J}_{23} =
\frac{\partial \mathbf{e}_{23}}{\partial \mathbf{\mathcal{X}}} = \Big[ [\mathbf{0}]_{2\times6}, \tfrac{\partial \mathbf{e}_{23}}{\partial \boldsymbol{\xi}_2}, [\mathbf{0}]_{2\times3}, [\mathbf{0}]_{2\times3}, \tfrac{\partial \mathbf{e}_{23}}{\partial \mathbf{X}_3} \Big]
\end{equation}

stack them together and get the whole jacobian matrix,
\begin{equation}
\mathbf{J} = 
\begin{bmatrix}
\mathbf{J}_{11} \\
\mathbf{J}_{12} \\
\mathbf{J}_{22} \\
\mathbf{J}_{23} \\
\end{bmatrix}
\end{equation}

Thus the Hessian matrix $\mathbf{H}$ can be derived as,
\begin{equation}
\mathbf{H} = \mathbf{J}^\top \mathbf{J} = 
\begin{bmatrix}
\mathbf{J}_{11} ^\top, \mathbf{J}_{12} ^\top, \mathbf{J}_{22} ^\top, \mathbf{J}_{23} ^\top \\
\end{bmatrix} 
\begin{bmatrix}
\mathbf{J}_{11} \\
\mathbf{J}_{12} \\
\mathbf{J}_{22} \\
\mathbf{J}_{23} \\
\end{bmatrix} 
= 
\sum_{i,j} \mathbf{J}_{ij}^\top \mathbf{J}_{ij}
\end{equation}

\begin{equation}
\mathbf{J}_{11}^\top \mathbf{J}_{11} =
\begin{bmatrix}
\left( \frac{\partial \mathbf{e}_{11}}{\partial \boldsymbol{\xi}_1} \right)^\top \frac{\partial \mathbf{e}_{11}}{\partial \boldsymbol{\xi}_1}, [\mathbf{0}]_{6\times6}, \left( \frac{\partial \mathbf{e}_{11}}{\partial \boldsymbol{\xi}_1} \right)^\top \frac{\partial \mathbf{e}_{11}}{\partial \mathbf{X}_1}, [\mathbf{0}]_{3\times3}, [\mathbf{0}]_{3\times3} \\
\mathbf{0}  \\
\left( \frac{\partial \mathbf{e}_{11}}{\partial \mathbf{X}_1} \right)^\top \frac{\partial \mathbf{e}_{11}}{\partial \boldsymbol{\xi}_1}, [\mathbf{0}]_{3\times6}, \left( \frac{\partial \mathbf{e}_{11}}{\partial \boldsymbol{\xi}_1} \right)^\top \frac{\partial \mathbf{e}_{11}}{\partial \mathbf{X}_1}, [\mathbf{0}]_{3\times3}, [\mathbf{0}]_{3\times3} \\
\mathbf{0} \\
\mathbf{0} \\
\end{bmatrix}
\end{equation}

\begin{equation}
\mathbf{J}_{12}^\top \mathbf{J}_{12} =
\begin{bmatrix}
\left( \frac{\partial \mathbf{e}_{12}}{\partial \boldsymbol{\xi}_1} \right)^\top
\frac{\partial \mathbf{e}_{12}}{\partial \boldsymbol{\xi}_1}
, [\mathbf{0}]_{6\times6}
, [\mathbf{0}]_{6\times3}
, \left( \frac{\partial \mathbf{e}_{12}}{\partial \boldsymbol{\xi}_1} \right)^\top
\frac{\partial \mathbf{e}_{12}}{\partial \mathbf{X}_2}
, [\mathbf{0}]_{6\times3} \\
\mathbf{0} \\
\mathbf{0} \\
\left( \frac{\partial \mathbf{e}_{12}}{\partial \mathbf{X}_2} \right)^\top
\frac{\partial \mathbf{e}_{12}}{\partial \boldsymbol{\xi}_1}
, [\mathbf{0}]_{3\times6}
, [\mathbf{0}]_{3\times3}
, \left( \frac{\partial \mathbf{e}_{12}}{\partial \mathbf{X}_2} \right)^\top
\frac{\partial \mathbf{e}_{12}}{\partial \mathbf{X}_2}
, [\mathbf{0}]_{3\times3} \\
\mathbf{0}
\end{bmatrix}
\end{equation}

\begin{equation}
\mathbf{J}_{22}^\top \mathbf{J}_{22} =
\begin{bmatrix}
\mathbf{0}
\\
[\mathbf{0}]_{6\times6}
, \left( \frac{\partial \mathbf{e}_{22}}{\partial \boldsymbol{\xi}_2} \right)^\top
\frac{\partial \mathbf{e}_{22}}{\partial \boldsymbol{\xi}_2}
, [\mathbf{0}]_{6\times3}
, \left( \frac{\partial \mathbf{e}_{22}}{\partial \boldsymbol{\xi}_2} \right)^\top
\frac{\partial \mathbf{e}_{22}}{\partial \mathbf{X}_2}
, [\mathbf{0}]_{6\times3}
\\
\mathbf{0}
\\
[\mathbf{0}]_{3\times6}
, \left( \frac{\partial \mathbf{e}_{22}}{\partial \mathbf{X}_2} \right)^\top
\frac{\partial \mathbf{e}_{22}}{\partial \boldsymbol{\xi}_2}
, [\mathbf{0}]_{3\times3}
, \left( \frac{\partial \mathbf{e}_{22}}{\partial \mathbf{X}_2} \right)^\top
\frac{\partial \mathbf{e}_{22}}{\partial \mathbf{X}_2}
, [\mathbf{0}]_{3\times3}
\\
\mathbf{0}
\end{bmatrix}
\end{equation}


\begin{equation}
\mathbf{J}_{23}^\top \mathbf{J}_{23} =
\begin{bmatrix}
\mathbf{0} \\
[\mathbf{0}]_{6\times6}
,\left( \frac{\partial \mathbf{e}_{23}}{\partial \boldsymbol{\xi}_2} \right)^\top
\frac{\partial \mathbf{e}_{23}}{\partial \boldsymbol{\xi}_2}
, [\mathbf{0}]_{6\times3}
, [\mathbf{0}]_{6\times3}
, \left( \frac{\partial \mathbf{e}_{23}}{\partial \boldsymbol{\xi}_2} \right)^\top
\frac{\partial \mathbf{e}_{23}}{\partial \mathbf{X}_3}
\\
\mathbf{0}
\\
\mathbf{0}
\\
[\mathbf{0}]_{3\times6}
, \left( \frac{\partial \mathbf{e}_{23}}{\partial \mathbf{X}_3} \right)^\top
\frac{\partial \mathbf{e}_{23}}{\partial \boldsymbol{\xi}_2}
, [\mathbf{0}]_{3\times3}
, [\mathbf{0}]_{3\times3}
, \left( \frac{\partial \mathbf{e}_{23}}{\partial \mathbf{X}_3} \right)^\top
\frac{\partial \mathbf{e}_{23}}{\partial \mathbf{X}_3}
\end{bmatrix}
\end{equation}

We define derivative terms as,
\begin{equation}
\mathbf{E}_{ij}
\triangleq
\frac{\partial \mathbf{e}_{ij}}{\partial \boldsymbol{\xi}_i}
\in \mathbb{R}^{2\times 6},
\qquad
\mathbf{F}_{ij}
\triangleq
\frac{\partial \mathbf{e}_{ij}}{\partial \mathbf{X}_j}
\in \mathbb{R}^{2\times 3}.
\end{equation}


And the Hessian $\mathbf{H}$ is,
\begin{aligned}
\mathbf{H}
&= \begin{bmatrix}
\mathbf{H}_{CC} & \mathbf{H}_{CP} \\
\mathbf{H}_{PC} & \mathbf{H}_{PP}
\end{bmatrix} \\
&=
\left[
\begin{array}{cc|ccc}
\mathbf{E}_{11}^\top\mathbf{E}_{11} + \mathbf{E}_{12}^\top\mathbf{E}_{12}
& [\mathbf{0}]_{6\times6}
& \mathbf{E}_{11}^\top\mathbf{F}_{11}
& \mathbf{E}_{12}^\top\mathbf{F}_{12}
& [\mathbf{0}]_{6\times3}
\\[6pt]

[\mathbf{0}]_{6\times6}
& \mathbf{E}_{22}^\top\mathbf{E}_{22} + \mathbf{E}_{23}^\top\mathbf{E}_{23}
& [\mathbf{0}]_{6\times3}
& \mathbf{E}_{22}^\top\mathbf{F}_{22}
& \mathbf{E}_{23}^\top\mathbf{F}_{23}
\\
\hline

\mathbf{F}_{11}^\top\mathbf{E}_{11}
& [\mathbf{0}]_{3\times6}
& \mathbf{F}_{11}^\top\mathbf{F}_{11}
& [\mathbf{0}]_{3\times3}
& [\mathbf{0}]_{3\times3}
\\[6pt]

\mathbf{F}_{12}^\top\mathbf{E}_{12}
& \mathbf{F}_{22}^\top\mathbf{E}_{22}
& [\mathbf{0}]_{3\times3}
& \mathbf{F}_{12}^\top\mathbf{F}_{12} + \mathbf{F}_{22}^\top\mathbf{F}_{22}
& [\mathbf{0}]_{3\times3}
\\[6pt]

[\mathbf{0}]_{3\times6}
& \mathbf{F}_{23}^\top\mathbf{E}_{23}
& [\mathbf{0}]_{3\times3}
& [\mathbf{0}]_{3\times3}
& \mathbf{F}_{23}^\top\mathbf{F}_{23}
\end{array}
\right]
\end{aligned}

It is obvious that the Hessian matrix is symmetric. The upper left block of the Hessian matrix $\mathbf{H}_{CC} \in \mathbb{R}^{6M \times 6M}$ is block diagonal and all about camera pose $\boldsymbol{\xi}$ and the lower right block $\mathbf{H}_{PP} \in \mathbb{R}^{3N \times 3N}$ is also block diagonal and all about 3D points $\mathbf{X}$. 

The effective size of the Hessian used for optimization depends on the observability between camera poses and 3D points. In fact, the observability information is also hidden in $\mathbf{H}$ itself. Think about the last block in the first row block of $\mathbf{H}$, it is a $[\mathbf{0}]$ block because the 3rd 3D landmark is not observable by the 1st camera. Same for the 3rd block in the second row block, it is $[\mathbf{0}]$ because the 1st 3D point is not observable by camera 2. <br>

In summary, the Jacobian of the reprojection error over the $i$-th camera pose and the $j$-th 3D landmark points are,
\begin{equation}
\mathbf{J}_{ij}(\mathcal{X}) =
\Big[
\underbrace{[\mathbf{0}]_{2\times6}\;\cdots\;[\mathbf{0}]_{2\times6}}_{\text{poses } 1\ldots i-1}\;\;
\tfrac{\partial \mathbf{e}_{ij}}{\partial \boldsymbol{\xi}_i}\;\;
\underbrace{[\mathbf{0}]_{2\times6}\;\cdots\;[\mathbf{0}]_{2\times6}}_{\text{poses } i+1\ldots M}\;\;
\underbrace{[\mathbf{0}]_{2\times3}\;\cdots\;[\mathbf{0}]_{2\times3}}_{\text{landmarks } 1\ldots j-1}\;\;
\tfrac{\partial \mathbf{e}_{ij}}{\partial \mathbf{X}_j}\;\;
\underbrace{[\mathbf{0}]_{2\times3}\;\cdots\;[\mathbf{0}]_{2\times3}}_{\text{landmarks } j+1\ldots N}
\Big]
\end{equation}

We use $\mathbf{\mathcal{C}}_j$ and $\mathbf{\mathcal{P}}_i$ to denote the set of observable cameras for the $j$-th 3D point and, observable 3D points for the $i$-th camera. We also use $| \mathbf{\mathcal{C}}_j |$ and $| \mathbf{\mathcal{P}}_i |$ to denote their sizes. And the Hessian matrix is,
\begin{equation}
\mathbf{H} =
\left[
\begin{array}{cccc|cccc}
\sum\limits_{j}^{| \mathbf{\mathcal{P}}_1 |} \mathbf{E}_{1j}^{\top}\mathbf{E}_{1j}
& \mathbf{0} & \cdots & \mathbf{0}
& \mathbf{E}_{11}^{\top}\mathbf{F}_{11}
& \mathbf{E}_{12}^{\top}\mathbf{F}_{12}
& \cdots
& \mathbf{E}_{1N}^{\top}\mathbf{F}_{1N}
\\

\mathbf{0}
& \sum\limits_{j}^{| \mathbf{\mathcal{P}}_2 |} \mathbf{E}_{2j}^{\top}\mathbf{E}_{2j}
& \cdots & \mathbf{0}
& \mathbf{E}_{21}^{\top}\mathbf{F}_{21}
& \mathbf{E}_{22}^{\top}\mathbf{F}_{22}
& \cdots
& \mathbf{E}_{2N}^{\top}\mathbf{F}_{2N}
\\

\vdots & \vdots & \ddots & \vdots
& \vdots & \vdots & \ddots & \vdots
\\

\mathbf{0} & \mathbf{0} & \cdots
& \sum\limits_{j}^{| \mathbf{\mathcal{P}}_M |} \mathbf{E}_{Mj}^{\top}\mathbf{E}_{Mj}
& \mathbf{E}_{M1}^{\top}\mathbf{F}_{M1} & \mathbf{E}_{M2}^{\top}\mathbf{F}_{M2} & \cdots
& \mathbf{E}_{MN}^{\top}\mathbf{F}_{MN}
\\
\hline

\mathbf{F}_{11}^{\top}\mathbf{E}_{11}
& \mathbf{F}_{21}^{\top}\mathbf{E}_{21} & \cdots &\mathbf{F}_{M1}^{\top}\mathbf{E}_{M1}
& \sum\limits_{i}^{| \mathbf{\mathcal{C}}_1 |} \mathbf{F}_{i1}^{\top}\mathbf{F}_{i1}
& \mathbf{0} & \cdots & \mathbf{0}
\\

\mathbf{F}_{12}^{\top}\mathbf{E}_{12}
& \mathbf{F}_{22}^{\top}\mathbf{E}_{22}
& \cdots & \mathbf{F}_{M2}^{\top}\mathbf{E}_{M2}
& \mathbf{0}
& \sum\limits_{i}^{| \mathbf{\mathcal{C}}_2 |} \mathbf{F}_{i2}^{\top}\mathbf{F}_{i2}
& \cdots & \mathbf{0}
\\

\vdots & \vdots & \ddots & \vdots
& \vdots & \vdots & \ddots & \vdots
\\

\mathbf{F}_{1N}^{\top}\mathbf{E}_{1N}
& \mathbf{F}_{2N}^{\top}\mathbf{E}_{2N}
& \cdots
& \mathbf{F}_{MN}^{\top}\mathbf{E}_{MN}
& \mathbf{0} & \mathbf{0} & \cdots
& \sum\limits_{i}^{| \mathbf{\mathcal{C}}_N |}  \mathbf{F}_{iN}^{\top}\mathbf{F}_{iN}
\end{array}
\right]
\end{equation}

Notes that part of the blocks in $\mathbf{H}_{CP}$ are actually $\mathbf{0}$ matrices, here we write them all explicitly for the sake of completeness. But the real $\mathbf{H}_{CP}$ matrix (contains how many $\mathbf{0}$ blocks, e.g. it's sparsity) depends on the observability for the $i-th$ camera and the $j-th$ points. And in practice, the size of 3D points $N$ is often much larger than the size of cameras $M$.

<h3>Schur Complement</h3>
As we discussed before, the inremental updates for $\mathbf{\mathcal{X}}$ can be solved by the following linear equation,
\begin{equation}
\begin{bmatrix}
\mathbf{H}_{CC} & \mathbf{H}_{CP} \\
\mathbf{H}_{CP}^\top & \mathbf{H}_{PP}
\end{bmatrix}
\begin{bmatrix}
\Delta \mathbf{\mathcal{X}}_C \\ \Delta \mathbf{\mathcal{X}}_P
\end{bmatrix}
=
\begin{bmatrix}
\mathbf{J}_C \mathbf{e}_C \\ \mathbf{J}_P \mathbf{e}_P
\end{bmatrix}
\end{equation}

We will use the internal sparsity of Hessian to make the solving more efficient. We want to avoid inversing the entire Hessian matrix by inversing only part of it. In specific, we want to inverse $\mathbf{H}$ by only inversing part of $\mathbf{H}$, ideally the diagonal part, and not $\mathbf{H}_{CP}$ and $\mathbf{H}_{CP}^\top$, which are usually more expensive to inverse than a diagonal matrix.
\begin{equation}
\begin{bmatrix}
\mathbf{I} & -\mathbf{H}_{CP}\mathbf{H}_{PP}^{-1} \\
\mathbf{0} & \mathbf{I}
\end{bmatrix}
\begin{bmatrix}
\mathbf{H}_{CC} & \mathbf{H}_{CP} \\
\mathbf{H}_{CP}^{\top} & \mathbf{H}_{PP}
\end{bmatrix}
\begin{bmatrix}
\Delta \boldsymbol{\mathcal{X}}_C \\
\Delta \boldsymbol{\mathcal{X}}_P
\end{bmatrix}
=
\begin{bmatrix}
\mathbf{I} & -\mathbf{H}_{CP}\mathbf{H}_{PP}^{-1} \\
\mathbf{0} & \mathbf{I}
\end{bmatrix}
\begin{bmatrix}
\mathbf{J}_C^{\top}\mathbf{e} \\
\mathbf{J}_P^{\top}\mathbf{e}
\end{bmatrix}
\end{equation}

what we do next is to multiply a matrix on both sides and eliminate the $\mathbf{H}_{CP}$ in the linear system, obtaining,
\begin{equation}
\begin{bmatrix}
\mathbf{H}_{CC}
- \mathbf{H}_{CP}\mathbf{H}_{PP}^{-1}\mathbf{H}_{CP}^{\top}
& \mathbf{0}
\\
\mathbf{H}_{CP}^{\top}
& \mathbf{H}_{PP}
\end{bmatrix}
\begin{bmatrix}
\Delta \boldsymbol{\mathcal{X}}_C \\
\Delta \boldsymbol{\mathcal{X}}_P
\end{bmatrix}
=
\begin{bmatrix}
\mathbf{J}_C^{\top}\mathbf{e}
- \mathbf{H}_{CP}\mathbf{H}_{PP}^{-1}\mathbf{J}_P^{\top}\mathbf{e}
\\
\mathbf{J}_P^{\top}\mathbf{e}
\end{bmatrix}
\end{equation}

then we get
\begin{equation}
\left(
\mathbf{H}_{CC}
- \mathbf{H}_{CP}\mathbf{H}_{PP}^{-1}\mathbf{H}_{CP}^{\top}
\right)
\Delta \boldsymbol{\mathcal{X}}_C
=
\mathbf{J}_C^{\top}\mathbf{e}
- \mathbf{H}_{CP}\mathbf{H}_{PP}^{-1}\mathbf{J}_P^{\top}\mathbf{e}
\end{equation}

and solve $\Delta \mathbf{\mathcal{X}_C}$ as,
\begin{equation}
\Delta \boldsymbol{\mathcal{X}}_C
=
\left(
\mathbf{H}_{CC}
- \mathbf{H}_{CP}\mathbf{H}_{PP}^{-1}\mathbf{H}_{CP}^{\top}
\right)^{-1} \left( \mathbf{J}_C^{\top}\mathbf{e}
- \mathbf{H}_{CP}\mathbf{H}_{PP}^{-1}\mathbf{J}_P^{\top}\mathbf{e} \right)
\end{equation}

Matrix $\mathbf{S} = \left(\mathbf{H}_{CC} - \mathbf{H}_{CP}\mathbf{H}_{PP}^{-1}\mathbf{H}_{CP}^{\top} \right)$ is called the Schur complement of $\mathbf{H}_{PP}$. Although $\mathbf{S} $ is neither really sparse nor diagonal, the size is relatively small ($6M \times 6M$) and its construction and solution are typically computationally inexpensive in practice. <br>

The incremental for the 3D points are then solved as,
\begin{equation}
\Delta \boldsymbol{\mathcal{X}}_P
=
\mathbf{H}_{PP}^{-1}
\left(
\mathbf{J}_P^{\top}\mathbf{e}
-
\mathbf{H}_{CP}^{\top}
\Delta \boldsymbol{\mathcal{X}}_C
\right)
\end{equation}

Taking a look back again, we solve $\Delta \boldsymbol{\mathcal{X}}_C$ by inversing a relative small, non-diagonal matrix, and then solve $\Delta \boldsymbol{\mathcal{X}}_P$ by inversing a large but sparse, diagonal matrix.

<h2>Pose graph introduction</h2>