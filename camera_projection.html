<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,400,900">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans:300,400,700">
    <link rel="stylesheet" href="../../latex.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
        },
        TeX: {
          equationNumbers: {
            autoNumber: "AMS",
            useLabelIds: true
          }
        },
        "HTML-CSS": {
          availableFonts: ["STIX"],
          linebreaks: { automatic: true },
          imageFont: null
        }
      });
    </script>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <meta charset="UTF-8">
    <base target="_blank">
    <title>Cameras, Sensors and Projection Model</title>
    <style>
        body {
            font-family: 'Lato', 'Google Sans', sans-serif;
            line-height: 1.6;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px 40px;
            font-size: 20px;
            counter-reset: figure-counter;
            text-align: justify;
        }
        h1 {
            color: #333;
            font-family: 'Google Sans', sans-serif;
            text-align: center;
            margin-bottom: 2em;
            font-size: 2.5em;
        }
        h2 {
            color: #333;
            font-family: 'Google Sans', sans-serif;
            font-size: 1.8em;
            margin-top: 1.5em;
        }
        h3 {
            color: #333;
            font-family: 'Google Sans', sans-serif;
            font-size: 1.4em;
            margin-top: 1.2em;
        }
        .figure {
            text-align: center;
            margin: 30px 0;
            width: 100%;
            counter-increment: figure-counter;
        }
        .figure img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 0 auto;
        }
        .figure-caption {
            font-style: italic;
            margin-top: 15px;
            text-align: center;
            font-size: 1em;
        }
        .figure-caption::before {
            content: "Figure " counter(figure-counter) ": ";
            font-weight: bold;
            font-style: normal;
        }
        .subfigure-container {
            display: flex;
            justify-content: center;
            gap: 40px;
            margin-bottom: 20px;
            max-width: 1400px;
            margin-left: auto;
            margin-right: auto;
        }
        .subfigure {
            flex: 0 1 600px;
            text-align: center;
        }
        .subfigure img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 0 auto;
            min-width: 400px;
        }
        .subfigure .figure-caption {
            font-style: italic;
            margin-top: 10px;
            text-align: center;
            font-size: 1em;
        }
        .subfigure .figure-caption::before {
            content: none;
        }
        .MathJax {
            font-size: 1.1em !important;
        }
        .MathJax_Display {
            overflow-x: auto;
            overflow-y: hidden;
            margin: 1em 0;
        }
        .equation-container {
            display: table;
            width: 100%;
            margin: 1.5em 0;
        }
        .equation-content {
            display: table-cell;
            width: 100%;
        }
        .figure-label::before {
            content: "Figure " counter(figure-counter) ":";
            font-weight: bold;
            margin-right: 0.5em;
        }
        /* MathJax display styles */
        .MJXc-display {
            overflow-x: auto;
            overflow-y: hidden;
            scrollbar-width: none;
            -ms-overflow-style: none;
        }
        .MJXc-display::-webkit-scrollbar {
            width: 5px;
            height: 2px;
        }
        .MJXc-display::-webkit-scrollbar-track {
            background: transparent;
        }
        .MJXc-display::-webkit-scrollbar-thumb {
            background: #ddd;
            visibility:hidden;
        }
        .MJXc-display:hover::-webkit-scrollbar-thumb {
            visibility:visible;
        }
    </style>

</head>
<body>
    <h1>Cameras, Sensors and Projection Model</h1>
    
    <h2>Pinhole Projection Model</h2>
    Pinhole camera projection model is the most common camera projection model. It is a simple model that projects a 3D point to a 2D image plane.
    <div class="figure">
        <img src="figs/pinhole_model.png" alt="Pinhole Camera Projection Model Illustraion", style="width: 90%;">
        <div class="figure-caption">Pinhole Camera Projection Model</div>
    </div>

    We first dicuss the projection of 3D points in camera coordinate system to 2D image points. Slice one place of the above 3D space,

    <div class="figure">
        <img src="figs/pinhole_model_one_plane.png" alt="Pinhole Camera Projection Model in 2D", style="width: 70%;">
        <div class="figure-caption">Pinhole Camera Projection Model in 2D</div>
    </div>

    With simple geometry rules, we find the camera intrinsic matrix as,
    \begin{equation}
    \frac{Z}{f_y} = \frac{Y}{y}
    \quad
    \quad
    \frac{Z}{f_x} = \frac{X}{x}
    \end{equation}

    \begin{equation}
    y = \frac{f_y Y}{Z}
    \quad
    \quad
    x = \frac{f_x X}{Z}
    \end{equation}

    which is a mapping from 3D Euclidean space to 2D Euclidean space,
    \begin{equation}
    \begin{bmatrix} 
    X \\
    Y \\
    Z \\
    \end{bmatrix} 
    \longrightarrow
    \begin{bmatrix} 
    \frac{f_x X}{Z} \\
    \\
    \frac{f_y Y}{Z} \\
    \end{bmatrix} 
    \end{equation}

    considering the principal point, the final coordinate of the projected 3D point in camera coordinate system $X_c$ is, which is denoted as $x_i$
    \begin{equation}
    \mathbf{x}_i =
    \begin{bmatrix}
    u \\
    v \\
    \end{bmatrix}
    =
    \begin{bmatrix} 
    \frac{f_x X}{Z} + c_x\\
    \\
    \frac{f_y Y}{Z} + c_y\\
    \end{bmatrix} 
    \end{equation}

    \begin{equation}
    \tilde {\mathbf{x}}_i
    =
    \begin{bmatrix}
    u \\
    v \\
    1 \\
    \end{bmatrix}
    =
    \begin{bmatrix} 
    \frac{f_x X}{Z} + c_x\\
    \\
    \frac{f_y Y}{Z} + c_y\\
    \\
    1 \\

    \end{bmatrix} 
    \end{equation}

    now we want to write it in matrix form,
    \begin{equation}
    \tilde {\mathbf{x}}_i = \mathbf{K} \mathbf{X}_C  
    \end{equation}

    \begin{equation}
    \begin{bmatrix} 
    f_x & 0 & c_x \\
    0 & f_y & c_y \\
    0 & 0 & 1 \\
    \end{bmatrix}  
    \begin{bmatrix}
    X \\
    Y \\
    Z \\
    \end{bmatrix} 
    =
    \begin{bmatrix}
    f_x X + c_x Z \\
    f_y Y + c_y Z \\
    Z
    \end{bmatrix}
    =
    Z
    \begin{bmatrix}
    f_x X \verb|/| Z + c_x \\
    f_y Y \verb|/| Z + c_y \\
    1
    \end{bmatrix}
    \end{equation}

    $\mathbf{K}$ is camera intrinsic matrix
    \begin{equation}
    \mathbf{K} = 
    \begin{bmatrix} 
    f_x & 0 & c_x \\
    0 & f_y & c_y \\
    0 & 0 & 1 \\
    \end{bmatrix} 
    \end{equation}

    Generally, we want to have all of the coordinates in homogeneous coordinates, which gives
    \begin{equation}
    \tilde {\mathbf{x}}_i = \mathbf{K} \tilde {\mathbf{X}}_C =
    \begin{bmatrix} 
    f_x & 0 & c_x & 0\\
    0 & f_y & c_y & 0 \\
    0 & 0 & 1 & 0\\
    \end{bmatrix} 
    \begin{bmatrix}
    X \\
    Y \\
    Z \\
    1
    \end{bmatrix} 
    \end{equation}

    <!-- However, this is not the 3D points in world frame, we need to inverse the camera extrinsic matrix to get the 3D points in world frame, combine the camera extrinsic together,
    \begin{equation}
    \tilde {\mathbf{x}}_i = \mathbf{K} \tilde {\mathbf{X}}_C 
    = \mathbf{K} \mathbf{T}_{CW}
    \tilde {\mathbf{X}}_W
    \end{equation} -->

    and matrix $\mathbf{T}_{CW}$ is the camera extrinsic matrix, which happens to be the inverse of $\mathbf{T}_{WC}$, the camera pose in world frame.

    If we want to recover the 3D points in camera frame, we can try to back project the 3D image point by inversing the camera intrinsic matrix,
    \begin{equation}
    \begin{aligned}
    \mathbf{X}_C &= \mathbf{K}^{-1} \tilde {\mathbf{x}}_i \\
    &= 
    \begin{bmatrix}
    \frac{1}{f_x} & 0 & -\frac{c_x}{f_x} \\
    0 & \frac{1}{f_y} & -\frac{c_y}{f_y} \\
    0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
    u \\
    v \\
    1
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
    \frac{u - c_x}{f_x} \\
    \frac{v - c_y}{f_y} \\
    1
    \end{bmatrix}
    \end{aligned}
    \end{equation}

    What is missing comparing with the original 3D points? We need the depth information $Z$, multiply the depth information we obtain,
    \begin{equation}
    \mathbf{X}_C =
    \begin{bmatrix}
    \frac{(u - c_x)Z}{f_x} \\
    \frac{(v - c_y)Z}{f_y} \\
    Z
    \end{bmatrix}
    \end{equation}

    How to get the depth information? We will need at least two views, or specific depth sensors.

    The full projection model also includes the rigid body transformation of camera frame seen from the world frame, denoted as $\mathbf{T}_{WC}$. But in camera projection model, we trasnform a 3D point in world frame to a 3D point in camera frame, making the convention using $\mathbf{T}_{CW}$ to denote the camera extrinsic matrix.

    \begin{equation}
    \tilde{\mathbf{X}}_C = \mathbf{T}_{CW} \tilde{\mathbf{X}}_W
    =
    \tilde{\mathbf{X}}_W
    \end{equation}

    here $\tilde{\mathbf{X}}_W$ is the 3D point in world frame in homogeneous coordinates.

    Combine the camera intrinsic matrix and the camera extrinsic matrix, we get the full projection model,
    \begin{equation}
    \mathbf{x} = \mathbf{P} \mathbf{X}_W = \mathbf{K} \mathbf{T}_{CW} \tilde{\mathbf{X}}_W
    \end{equation}

    <h2>Camera Distortion Model</h2>
    The real camera is usually not a perfect pinhole camera, it has some distortions. We model two common distortion models, the radial distortion and the tangential distortion.

    <h3>Radial Distortion</h3>
    The radial distortiono ccurs when the lens is not perfectly spherical. It is caused by the lens being too close to the image sensor.
    it is modeled by parameters $k_1$, $k_2$, $k_3$ as,
    \begin{equation}
    x_{\text{distorted}} = x(1 + k_1 r^2 + k_2 r^4 + k_3 r^6) \\
    y_{\text{distorted}} = y(1 + k_1 r^2 + k_2 r^4 + k_3 r^6)
    \end{equation}

    <h3>Tangential Distortion</h3>
    The tangential distortion is caused by lens misalignment, hence not perfectly parallel to image plane. It is illustrated as the following figure.
    <div class="figure">
        <img src="figs/tan_distortion.png" alt="Tangential Distortion Illustration" style="width: 60%;">
        <div class="figure-caption">Tangential Distortion Illustration</div>
    </div>

    The tangential distortion is modeled by parameters $p_1$ and $p_2$,

    \begin{equation}
    x_{\text{distorted}} = x + 2p_1 xy + p_2 (r^2 + 2x^2) \\
    y_{\text{distorted}} = y + p_1 (r^2 + 2y^2) + 2p_2 xy
    \end{equation}

    Putting the two distortion models together, we get a joint model with 5 distortion coefficients, 
    \begin{equation}
    \left\{
    \begin{aligned}
    x_{\text{distorted}} &= x(1 + k_1 r^2 + k_2 r^4 + k_3 r^6) + 2p_1 xy + p_2 (r^2 + 2x^2) \\
    y_{\text{distorted}} &= y(1 + k_1 r^2 + k_2 r^4 + k_3 r^6) + p_1 (r^2 + 2y^2) + 2p_2 xy
    \end{aligned}
    \right.
    \end{equation}



    <h2>Stereo Camera and Stereo Matching</h2>
    As we mentioned before, camera projection will lose the depth information. One way to recover the depth information is to use stereo cameras via a process called stereo matching. We will now derive the math model of stereo matching.
    The key concepts of stereo matching are denoted as the following figure.
    <div class="figure">
        <img src="figs/stereo_model.png" alt="Stereo Matching Math Model">
        <div class="figure-caption">Stereo Matching Math Model</div>
    </div>

    Based on simple geometry, we can find the the relationship of depth $Z$, disparity $d$, baseline $B$ and focal length $f$ as,
    \begin{equation}
    \begin{aligned}
    \frac{d}{f} = \frac{B}{Z} \\
    Z = \frac{B f}{d}
    \end{aligned}
    \end{equation}

    where $d$ is the disparity, $B$ is the baseline, $f_x$ is the focal length, $Z$ is the depth. The disparity is the difference in the x-coordinates of the same point projected onto the two cameras.


    <h2>Common Sensors used in Visual SLAM</h2>

    <h3>Stereo Cameras</h3>
    Many exsiting visual-(inertial) SLAM systems, such as  <a href="https://github.com/UZ-SLAMLab/ORB_SLAM3">ORB-SLAM3</a> and <a href="https://github.com/ethz-mrl/okvis2/">okvis2</a> can be used in the following commercial sensors. They are stereo cameras usually with a buit-in IMU inside them. Here is a table summarizing the sensors and their features.
    
    <table border="1" style="width: 100%; text-align: center;">
        <tr>
            <th style="padding: 15px;">Sensor</th>
            <th style="padding: 15px;">Depth Method</th>
            <th style="padding: 15px;">Built-in SLAM</th>
        </tr>
        <tr>
            <td style="padding: 15px;"><a href="https://www.intelrealsense.com/depth-camera-d455/">RealSense D455</a></td>
            <td style="padding: 15px;">Active IR Stereo</td>
            <td style="padding: 15px;">No</td>
        </tr>
        <tr>
            <td style="padding: 15px;"><a href="https://docs.luxonis.com/hardware/products/OAK-D%20Pro/">OAK-D Pro</a></td>
            <td style="padding: 15px;">Active IR Stereo</td>
            <td style="padding: 15px;">No</td>
        </tr>
        <tr>
            <td style="padding: 15px;"><a href="https://www.stereolabs.com/zed-2/">ZED 2(i)</a></td>
            <td style="padding: 15px;">Learning-based Stereo</td>
            <td style="padding: 15px;">Yes</td>
        </tr>
    </table>

    If we observe, for example, a Realsense D455 camera, we will see the two grayscale cameras on the most left and right side of the camera. The middle one is the IR projector and a RGB camera. 
    <div class="figure">
        <img src="figs/d455.jpg" alt="Realsense D455">
        <div class="figure-caption">Realsense D455</div>
    </div>
    
    The IR projector is used to project the IR pattern, e.g. dots, into un-textured regions, to improve the robustness of stereo matching for depth computation.
    <div class="figure">
        <img src="figs/rs_IR.png" alt="Realsense IR Projector Patterns" style="width: 70%;">
        <div class="figure-caption">Realsense IR Projector Dots Patterns (Source: <a href="https://mag.switch-science.com/2018/04/19/intel-realsense-depth-camera-d435-handson/"> from here</a>)</div>
    </div>

    <h3>Time-of-Flight (ToF) Cameras</h3>
    Another type of common camera is the Time-of-Flight (ToF) camera. It provides more accurate depth measurements, but its measurement range is often shorter, usually up to 5 meters. As the name suggests, ToF sensor emits IR light and measures the time it takes for light to travel to objects and back, and then calculate the distance to the objects.
    
    <div class="figure">
        <img src="figs/tof_principle.png" alt="ToF Camera Principle" style="width: 50%;">
        <div class="figure-caption">ToF Camera Principle (Image from <a href="https://en.wikipedia.org/wiki/Time-of-flight_camera">Wikipedia page</a>)</div>
    </div>
    
    Azure Kinect was a popular ToF camera, but unfortunatly it is now discontinued. However, we still have many good choices for ToF sensors, such as <a href="https://shop.luxonis.com/products/oak-d-sr-poe?srsltid=AfmBOoqWU0xM3ISEal6QwmXU3Xn9iqoW7QDNvXg5GdaravQoiHXsxHC9">OAK-D TOF</a>, <a href="https://www.orbbec.com/products/tof-camera/femto-bolt/" >Orbbec Femto Bolt</a>, etc. We can check out these videos to see the depth quality of Orbbec Femto Bolt. <a  href="https://www.youtube.com/watch?v=YSlnLfsPIuM&ab_channel=JetsonHacks">here</a> and <a href="https://www.youtube.com/watch?v=uM8bDCaruOs&ab_channel=Orbbec">here</a>.

    In fact, LiDAR works in a similar principle as ToF cameras. The difference is that LiDAR uses a laser pulse with higher frequency, and it can reach to much longer distances.

    <h2>References</h2>
    <ol>
        <li>Visual SLAM: From Theory to Practice, by Xiang Gao, Tao Zhang, Qinrui Yan and Yi Liu</li>
        <li>Multiview Geometry in Computer Vision, by Richard Hartley and Andrew Zisserman</li>
    </ol>

<br>
If you find this article helpful, please consider citing it in your research:
<pre style="background-color: #f5f5f5; padding: 15px; border-radius: 5px; overflow-x: auto;">
@article{Hu2026learnvslam,
  author = {Hu, Yafei},
  title = {Learning Visual SLAM: Cameras, Sensors and Projection Model},
  year = {2026}
}
</pre>

</body>
